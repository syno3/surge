# -*- coding: utf-8 -*-
"""TrafficSignDetector.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W0GRSxNgXtNEOki8rPYrI9DEB_XXBKTA
"""

### we import modules
try:
  import numpy as np
  import pandas as pd
  import cv2 as cv
  import tensorflow as tf
  import logging
  import matplotlib.pyplot as plt
  import os
  import PIL


  from tensorflow import keras
  from tensorflow.keras import layers
  from tensorflow.keras.models import Sequential
  import pathlib

  from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout
  from keras.models import Sequential
except Exception as e:
  logging.warning('{}'.format(e))

## initialzie some variables
path = '/content/traffic-signs-classification/myData'
labels = '/content/traffic-signs-classification/labels.csv'

data_dir = pathlib.Path(path)

image_count = len(list(data_dir.glob('*/*.jpg')))
print('there are {} images'.format(image_count))

### some parameter for the loader
batch_size = 32
img_height = 180
img_width = 180

def preprocessing(images):
  image = cv.imread(images)
  image = cv.cvtColor(image, cv2.COLOR_BGR2GRAY)
  return image

### splitting the data (tarining dataset)
train_ds = tf.keras.preprocessing.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset="training",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)

## validation dataset

val_ds = tf.keras.preprocessing.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset="validation",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)

class_names = train_ds.class_names
print(class_names)

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 10))
for images, labels in train_ds.take(1):
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(images[i].numpy().astype("uint8"))
    plt.title(class_names[labels[i]])
    plt.axis("off")

for image in train_ds:
  pass

## print image batch, size of images and channels
for image_batch, labels_batch in train_ds:
  print(image_batch.shape)
  print(labels_batch.shape)
  break

### buffered prefetching so you can yield data from disk without having I/O become blocking

AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

### standardize dataset

normalization_layer = layers.experimental.preprocessing.Rescaling(1./255)

num_classes = len(class_names)

model = Sequential([
  layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 3)),
  layers.Conv2D(16, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(32, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(64, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Flatten(),
  layers.Dense(128, activation='relu'),
  layers.Dense(num_classes)
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

model.summary()